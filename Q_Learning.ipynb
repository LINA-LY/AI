{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdqDbXzeffobU9Awqhsj4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LINA-LY/AI/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwYXn6XjMxw2",
        "outputId": "3b5d11e1-c0fb-4341-af5b-f490ca033a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V38WnArnU-4Z",
        "outputId": "d9165556-98ca-4451-84c2-c12907855430"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
        "state, _ = env.reset()\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 25000\n",
        "SHOW_EVERY = 2000\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
        "print(discrete_os_win_size)\n",
        "\n",
        "epsilon = 0.5\n",
        "START_EPSILON_DECAYING = EPISODES // 2\n",
        "END_EPSILON_DECAYING = EPISODES  # Fin de la dÃ©croissance de l'epsilon\n",
        "\n",
        "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(int))\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        print(episode)\n",
        "        render = True\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    discrete_state = get_discrete_state(state)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = 2  # always push right (for now)\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "        env.render()\n",
        "\n",
        "        if not done:\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "            current_q = q_table[discrete_state + (action, )]\n",
        "\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "            q_table[discrete_state + (action, )] = new_q\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            print(f\"We made it on episode {episode}\")\n",
        "            q_table[discrete_state + (action, )] = 0  # goal reward\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    # Decay epsilon\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon -= epsilon_decay_value\n",
        "        epsilon = max(epsilon, 0.1)  # Prevent epsilon from going below 0.1\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "iIWxvyFWSc38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOfrvvAbiPMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}